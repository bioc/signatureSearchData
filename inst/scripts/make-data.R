########################
## make cmap database ##
########################

# The `cmap` database represents the $log_2$ fold change of 12,437 genes 
# from 1,281 compound treatments in 4 cells (3,478 signatures in total).

# Download data from Connectivity Map project site at
# < https://portals.broadinstitute.org/cmap/>, which is the original CEL files 
# generated by Affymetrix chips. 
# The downloaded CEL files were normalized by MAS5 method and the differential 
# expression analysis from 'limma' package was used to get the `logMA` matrix 
# containing the log2FC scores. The code used for DE analysis is vailable at 
# the Longevity Tools (LT) website:
# <http://girke.bioinformatics.ucr.edu/longevityTools/mydoc/mydoc_longevityTools_eDRUG_01.html>.
# 
# The `degList.rds` generated from the above vignette is used to generate the 
# cmap log2FC database
library(SummarizedExperiment)
library(HDF5Array)
source("fct.R") # at the same place as this R script, contains some functions used below 
degList <- readRDS("degList.rds")
logMA <- degList$logFC
pert_cell_factor = gsub("(^.*)_(.*$)", "\\1__\\2__trt_cp", colnames(logMA))
colData <- DataFrame(pert_cell_factor=pert_cell_factor,
                     pert_cell = colnames(logMA))
new <- as.data.frame(t(sapply(seq_len(nrow(colData)), function(i) 
  unlist(strsplit(as.character(colData$pert_cell[i]), "_")))), 
  stringsAsFactors=FALSE)
colnames(new) = c("pert", "cell")
colData <- cbind(colData, as(new, "DataFrame"))
rownames(colData) = colData$pert_cell_factor
colnames(logMA)=colData$pert_cell_factor # Important to match format of lincs
cmap <- SummarizedExperiment(assays = list(score=logMA), colData = colData)
## Save 'cmap' as an HDF5-based SummarizedExperiment object on disk. 
## Here, it will save to the present working directory of the user’s R session.
cmap <- saveHDF5SummarizedExperiment(cmap, "./cmap")

#############################
## make cmap_expr database ##
#############################

# The `cmap_expr` database represents mean expression values of 1,309 drug
# treatment samples in 4 cells (3,587 signatures in total).

# For correlation based methods, the signature database containing gene 
# expression values from drug treatment samples could also be used to search 
# for similarity. 
# The `cmap_expr` database containing the rma normalized gene expression data.
# Then, it takes mean expression values of drug treatment samples at different 
# concentration, duration as expression values of that drug in a specific cell.

## Code used to generate the rma normalized expression dataset is similar to 
## the vignette at LT website with some modifications

### Download data from Connectivity Map project site
library(longevityTools)
getCmapCEL(rerun = FALSE) # Download cmap CEL files, will take some time
celfiles <- list.files("./data/CEL", pattern=".CEL$")
### Normalization of CEL files
chiptype <- readRDS("./data/chiptype.rds") # download `chiptype.rds` from LT.
chiptype_list <- split(names(chiptype), as.character(chiptype))
normalizeCel(chiptype_list, rerun=FALSE) # need to allocate a lot of memory, 512Gb works
df1 <- read.delim("./data/HG-U133A/rma_exprs.xls", sep="\t", header=T, 
                  row.names=1, check.names=FALSE)
df2 <- read.delim("./data/HT_HG-U133A/rma_exprs.xls", sep="\t", header=T, 
                  row.names=1, check.names=FALSE)
df3 <- read.delim("./data/U133AAofAv2/rma_exprs.xls", sep="\t", header=T, 
                  row.names=1, check.names=FALSE)
affyid <- rownames(df1)[rownames(df1) %in% rownames(df2)]
affyid <- affyid[affyid %in% rownames(df3)]
rmadf <- cbind(df1[affyid,], df2[affyid,], df3[affyid,])
### Obtain annotation information
library(hgu133a.db)
myAnnot <- data.frame(ACCNUM=sapply(contents(hgu133aACCNUM), paste, collapse=", "),
                      SYMBOL=sapply(contents(hgu133aSYMBOL), paste, collapse=", "),
                      UNIGENE=sapply(contents(hgu133aUNIGENE), paste, collapse=", "),
                      ENTREZID=sapply(contents(hgu133aENTREZID), paste, collapse=", "),
                      ENSEMBL=sapply(contents(hgu133aENSEMBL), paste, collapse=", "),
                      DESC=sapply(contents(hgu133aGENENAME), paste, collapse=", "))
write.table(myAnnot, file="./results/myAnnot.xls", quote=FALSE, sep="\t", col.names = NA)
saveRDS(myAnnot, "./results/myAnnot.rds")
### Transform probe set to gene level data
myAnnot <- readRDS("./results/myAnnot.rds") 
myAnnot <- myAnnot[as.character(myAnnot[,"ENTREZID"]) != "NA",]
rmadf <- rmadf[rownames(myAnnot),]
idlist <- tapply(row.names(myAnnot), as.character(myAnnot$ENTREZID), c)
rmadf <- t(sapply(names(idlist), function(x) colMeans(rmadf[idlist[[x]], ])))
saveRDS(rmadf,"./data/rmadf.rds")

## Generate `cmap_expr` database from `rmadf`
rmadf <- readRDS("./data/rmadf.rds") # matrix: 12437 7056
cmap_inst <- read.delim("./data/cmap_instances_02.txt", 
                        check.names=FALSE) # Download from LT website.
cmap_inst <- data.frame(cmap_inst, 
                        drug_cell = paste(cmap_inst$cmap_name, 
                                          cmap_inst$cell2, sep ="_"))
celid <- paste0(cmap_inst$perturbation_scan_id,".CEL")
sum(celid %in% colnames(rmadf)) # 5408
celid2 <- gsub("'", "", celid) # get rid of "'" in celid
sum(celid2 %in% colnames(rmadf)) # 6100
drug_celid_list <- tapply(celid2, cmap_inst$drug_cell, c) # 3587
cmap_drug_cell_expr <- sapply(names(drug_celid_list), function(x) 
  rowMeans(as.data.frame(rmadf[,drug_celid_list[[x]]]))) # 12437 X 3587
saveRDS(cmap_drug_cell_expr, "./data/cmap_drug_cell_expr.rds")
cmap_drug_cell_expr <- readRDS("./data/cmap_drug_cell_expr.rds")
## match format of colnames of "cmap_drug_cell_expr" as "pert__cell__trt_cp" 
pert_cell_factor = gsub("(^.*)_(.*$)", "\\1__\\2__trt_cp", 
                        colnames(cmap_drug_cell_expr))
colnames(cmap_drug_cell_expr) <- pert_cell_factor
cmap_expr <- SummarizedExperiment(assays = list(expr=cmap_drug_cell_expr))
## Save 'cmap_expr' as an HDF5-based SummarizedExperiment object:
cmap_expr <- saveHDF5SummarizedExperiment(cmap_expr, "./data/cmap_expr")

#########################
## make lincs database ##
#########################

# The `lincs` database represents moderate z-scores from differential expression 
# (DE) analysis of 12,328 genes from 8,140 compound treatments in 30 cells 
# (45,956 signatures in total). 

# Download the original LINCS datasets at GEO 
# <https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE92742>
# The needed files are:
#   + GSE92742_Broad_LINCS_Level5_COMPZ.MODZ_n473647x12328.gctx.gz
#   + GSE92742_Broad_LINCS_gene_info.txt.gz
#   + GSE92742_Broad_LINCS_sig_info.txt.gz
#   + GSE92742_Broad_LINCS_cell_info.txt.gz	
# Unzip the downloaded files to the `data` direcotry under the present working 
# directory of the user’s R session.

# The following codes require large number of memory in the system, 50Gb works
# Install cmapR package at <https://github.com/cmap/cmapR>
library(readr); library(dplyr); library(magrittr); library(cmapR)
library(SummarizedExperiment); library(HDF5Array)
meta42 <- read_tsv("./data/GSE92742_Broad_LINCS_sig_info.txt") 
txt <- names(table(meta42$pert_idose))
unit <- txt[grep("10 .M", txt)][2]
meta42 %<>% filter(pert_type=="trt_cp" & pert_idose==unit & pert_itime=="24 h") 
# filter rows by cmp, concentration, and treatment time
meta42 %<>% 
  bind_cols(alt_id=paste(meta42$pert_iname, meta42$cell_id, sep="_")) %>%
  bind_cols(pert_cell_factor=paste(meta42$pert_iname, meta42$cell_id, 
                                   meta42$pert_type, sep="__")) %>% 
  distinct(alt_id, .keep_all=TRUE) # eliminate technical duplicates
dim(meta42) # 45956 X 14
lincs42_mat <- cmapR::parse.gctx(
  "./data/GSE92742_Broad_LINCS_Level5_COMPZ.MODZ_n473647x12328.gctx", 
  cid=meta42$sig_id, matrix_only=TRUE)
lincs42_mat <- lincs42_mat@mat
colnames(lincs42_mat) = meta42$pert_cell_factor 
geneInfo <- read_tsv("./data/GSE92742_Broad_LINCS_gene_info.txt")
geneInfo <- as(geneInfo, "DataFrame")
geneInfo$pr_gene_id <- as.character(geneInfo$pr_gene_id)
rownames(geneInfo) <- geneInfo$pr_gene_id
geneInfo <- geneInfo[rownames(lincs42_mat),]
meta42 <- as(meta42, "DataFrame")
rownames(meta42) <- meta42$pert_cell_factor
lincs42 <- SummarizedExperiment(assays = list(score = lincs42_mat), 
                                rowData = geneInfo, colData = meta42)
# Get cell type information
cell_info <- read_tsv("./data/GSE92742_Broad_LINCS_cell_info.txt")
cell_name <- unique(meta42$cell_id)
cell_info %<>% dplyr::filter(base_cell_id %in% cell_name & !duplicated(
  paste(cell_info$base_cell_id, cell_info$sample_type, sep="_"))) %>%
  dplyr::select(c("cell_id","sample_type","primary_site","subtype")) %>% 
  dplyr::rename(cell_type=sample_type)
## replace "primary" as "normal" in cell_type column
cell_info %<>% mutate(cell_type=gsub("primary","normal",cell_type))
# Add cell info to lincs42 database
metadata(lincs42)$cell_info = cell_info
# Save 'lincs42' as an HDF5-based SummarizedExperiment object:
lincs42 <- saveHDF5SummarizedExperiment(lincs42, "./data/lincs")

##############################
## make lincs_expr database ##
##############################

# The `lincs_expr` database contains gene expression intensity values 
# from 5,925 compound treatments in 30 cells (38,824 signatures in total). 

# Download the original LINCS expression datasets at GEO 
# <https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE92742>
# The needed files are:
#   + GSE92742_Broad_LINCS_Level3_INF_mlr12k_n1319138x12328.gctx.gz
#   + GSE92742_Broad_LINCS_gene_info.txt.gz
#   + GSE92742_Broad_LINCS_inst_info.txt.gz
# Unzip the downloaded files to the `data` direcotry under the present working 
# directory of the user’s R session.

# The following codes require large number of memory in the system, 50Gb works
inst42 <- read_tsv("./data/GSE92742_Broad_LINCS_inst_info.txt") 
inst42 %<>% filter(pert_type=="trt_cp" & pert_dose==10 & pert_dose_unit=="um" 
                   & pert_time==24 & pert_time_unit=="h")
inst42 %<>% 
  bind_cols(alt_id=paste(inst42$pert_iname, inst42$cell_id, sep="_")) %>% 
  bind_cols(pert_cell_factor=paste(inst42$pert_iname, inst42$cell_id, 
                                   inst42$pert_type, sep="__"))
dim(inst42) # 169,795 X 13
lincs42_mat3 <- cmapR::parse.gctx(
  "./data/GSE92742_Broad_LINCS_Level3_INF_mlr12k_n1319138x12328.gctx", 
  cid=inst42$inst_id, matrix_only=TRUE)
lincs42_mat3 <- lincs42_mat3@mat # 12328 X 169795
pert_cell_list <- split(inst42$inst_id, inst42$pert_cell_factor)
lincs_drug_cell_expr <- sapply(names(pert_cell_list), function(x) 
  rowMeans(as.data.frame(lincs42_mat3[,pert_cell_list[[x]]]))) # 12328 X 38824
saveRDS(lincs_drug_cell_expr, "./data/lincs_drug_cell_expr.rds")
geneInfo <- read_tsv("./data/GSE92742_Broad_LINCS_gene_info.txt")
geneInfo <- as(geneInfo, "DataFrame")
geneInfo$pr_gene_id <- as.character(geneInfo$pr_gene_id)
rownames(geneInfo) <- geneInfo$pr_gene_id
geneInfo <- geneInfo[rownames(lincs_drug_cell_expr),]
inst42_uniq <- inst42[!duplicated(inst42$pert_cell_factor),] # 38824 X 13
inst42_uniq <- as(inst42_uniq, "DataFrame")
rownames(inst42_uniq) <- inst42_uniq$pert_cell_factor
lincs42_expr <- SummarizedExperiment(
  assays = list(expr = lincs_drug_cell_expr), 
  rowData = geneInfo, 
  colData = inst42_uniq[colnames(lincs_drug_cell_expr),])
## Save 'lincs42_expr' as an HDF5-based SummarizedExperiment object:
lincs42_expr <- saveHDF5SummarizedExperiment(lincs42_expr, "./data/lincs_expr")

################################
## make dtlink_db_clue_sti.db ##
################################

# This SQLite database contains drug-target links (dtlink) in DrugBank, CLUE, 
# and STITCH databases

## Get dtlink in DrugBank database (version 5.1.2)

### Download the `drugbank_all_target_uniprot_links.csv.zip` at 
### <https://www.drugbank.ca/releases/latest#external-links> under 
### `Target Drug-UniProt Links` section where `DRUG GROUP` is `All`.
### Extract the downloaded zip file, rename the extracted csv file as 
### `drug_target_uniprot_links_5.1.2.csv`, save it under `data` directory 

dt_uni_link <- read.delim("./data/drug_target_uniprot_links_5.1.2.csv",
                        sep=",", stringsAsFactors=FALSE)
#### Add gn_sym column for dt_uni_link
library(org.Hs.eg.db)
k <- keys(org.Hs.eg.db, keytype = "UNIPROT")
uni_gnsym <- suppressMessages(AnnotationDbi::select(
  org.Hs.eg.db, keys=k, columns=c("UNIPROT", "SYMBOL"), keytype="UNIPROT"))
dt_uni_link <- dplyr::as_tibble(dt_uni_link)
dt_uni_sym <- dplyr::left_join(dt_uni_link[,c("Name","UniProt.ID")], 
                               uni_gnsym, by = c("UniProt.ID"="UNIPROT"))
dtlink_db <- na.omit(unique(data.frame(drug_name=tolower(dt_uni_sym$Name), 
                        t_gn_sym=dt_uni_sym$SYMBOL, stringsAsFactors=FALSE)))
#### exclude drugs that have more than 100 targets in dtlink_db
dtlist_db <- split(dtlink_db$t_gn_sym, dtlink_db$drug_name)
dtnum_db <- sapply(dtlist_db, length)
sum(dtnum_db>100)
dtlist_db <- dtlist_db[dtnum_db<=100]
dtlink_db <- list2link(dtlist_db, type="dt")
#### exclude proteins that are targeted by more than 100 drugs in dtlink_db
tdlist_db <- split(dtlink_db$drug_name, dtlink_db$t_gn_sym)
tdnum_db <- sapply(tdlist_db, length)
sum(tdnum_db>100)
tdlist_db <- tdlist_db[tdnum_db<=100]
tdlink_db <- list2link(tdlist_db, type="td")
dtlink_db <- tdlink_db[,c("drug_name","t_gn_sym")]
length(unique(dtlink_db$drug_name)) # 5250
length(unique(dtlink_db$t_gn_sym)) # 2443
write.table(dtlink_db, "./data/dtlink_db_5.1.2.xls", 
            quote=FALSE, row.names=FALSE, sep="\t")

## Get dtlink in CLUE touchstone database at <https://clue.io/touchstone>
### Get pertabation information in CLUE touchstone (version 1.1.1.2) from API

#### Query the CLUE website at <https://clue.io/query> to get drugs in 
#### Touchstone database:
#### At Query window, select `Gene expression (L1000)` as query parameters. 
#### Select `individual query` as query mode. Then click the `example` link and
#### choose `PI3K/MTOR inhibitor` as an example query, the name of your query,
#### UP-regulated genes and DOWN-regulated genes would be automatically filled.
#### Then click `SUBMIT` button.
#### At the Query History window, select the query result, then click the 
#### `DETAILED LIST` button
#### At the result window, select `Compound x 2429` under `Perturbagen Type`.
#### Click `Export` button to save the result as `clue_example_mtor_res.txt` 
#### to your data directory under current R session. 

clue_res <- read.delim("./data/clue_example_mtor_res.txt",sep="\t") # 2429 X 6
clue_drugs <- as.character(unique(clue_res$Name)) # 2397
clue_pert_info_api <- getPertInfo(lincs_drugs)
saveRDS(clue_pert_info_api, "./data/clue_pert_info_api.rds")
clue_dt <- clue_pert_info_api[,c("pert_iname","target")]
clue_dtlist <- sapply(split(clue_dt$target, clue_dt$pert_iname), 
                       function(x) unique(unlist(x))) # 2397
dtlink_clue <- list2link(clue_dtlist, type="dt")
dtlink_clue$drug_name <- tolower(dtlink_clue$drug_name)
#### exclude drugs that have more than 100 targets, 
#### then exclude genes that have more than 100 targeted drugs
dtlist_clue <- split(dtlink_clue$t_gn_sym, dtlink_clue$drug_name)
dtnum_clue <- sapply(dtlist_clue, length)
sum(dtnum_clue > 100) # 0

tdlist_clue <- split(dtlink_clue$drug_name, dtlink_clue$t_gn_sym)
tdnum_clue <- sapply(tdlist_clue, length)
sum(tdnum_clue > 100)
tdlist_clue <- tdlist_clue[tdnum_clue <= 100]
tdlink_clue <- list2link(tdlist_clue, type="td")
dtlink_clue <- tdlink_clue[,c("drug_name","t_gn_sym")]
length(unique(dtlink_clue$drug_name)) # 1949
length(unique(dtlink_clue$t_gn_sym)) # 1325
write.table(dtlink_clue, "data/dtlink_clue_1.1.1.2.xls", 
            quote=FALSE, sep="\t", row.names=FALSE)

## Get dtlink in STITCH database (v5.0) 
### At STITCH Download page 
### <http://stitch.embl.de/cgi/download.pl?UserId=Z159fH9RYlUj&sessionId=2yGVo2kYC0ri>,
### Choose `Homo sapiens` organism, download and decompress the 
### `9606.protein_chemical.links.detailed.v5.0.tsv.gz` and
### `chemicals.v5.0.tsv.gz` as 
### `9606_protein_chemical_links_detailed_v5.0.tsv` and
### `chemicals.v5.0.tsv`, respectivily, to the data directory 
### under your current working directory of R session. 

### Need to allocate large number of memory to read the table, 20Gb works
sti_dtlink_detail <- readr::read_tsv(
  "data/9606_protein_chemical_links_detailed_v5.0.tsv") # 15,473,939 X 7
sti_dtlink <- dplyr::filter(sti_dtlink_detail, 
                            experimental > 700 | database > 700) # 346,909 X 7
### Eliminate drugs that have more than 100 targets in `sti_dtlink`
sti_dtlist <- split(sti_dtlink$protein, sti_dtlink$chemical)
sti_dtnum <- sapply(sti_dtlist, length)
invalid_drugs <- names(sti_dtlist[sti_dtnum>100]) # 356
sti_dtlink <- dplyr::filter(sti_dtlink, ! chemical %in% invalid_drugs)
sti_dtlink$protein <- sub("9606.", "", sti_dtlink$protein) # 227,948 X 7
### get PubChem CID to drug name mappings from `chemicals.v5.0.tsv` file
sti_chem_info <- readr::read_tsv("data/chemicals.v5.0.tsv")
### Map Ensembl protein ids to gene symbol 
library(EnsDb.Hsapiens.v75)
edb <- EnsDb.Hsapiens.v75
gnames <- keys(edb, keytype = "GENENAME")
ensb_gn_protid <- na.omit(select(edb, keys = gnames, keytype = "GENENAME", 
                                 columns = c("GENENAME", "PROTEINID")))
sti_dtlink %<>% left_join(ensb_gn_protid, by = c("protein" = "PROTEINID")) %>% 
    left_join(sti_chem_info[,c("chemical","name")]) %>% 
    dplyr::select(drug_name=name, t_gn_sym=GENENAME) # 227,948 X 2
sti_dtlink <- unique(na.omit(sti_dtlink)) # 142291 X 2
sti_dtlink$drug_name <- sapply(sti_dtlink$drug_name, 
                               function(x) unlist(strsplit(x,", "))[1])
sti_dtlink$drug_name <- tolower(sti_dtlink$drug_name)
### Exclude proteins that are targeted by more than 100 drugs in sti_dtlink
#### check whether there are drugs that have more than 100 targets 
#### after ENSP id to gene SYMBOL mappings
dtlist_sti <- split(sti_dtlink$t_gn_sym, sti_dtlink$drug_name)
dtnum_sti <- sapply(dtlist_sti, length)
sum(dtnum_sti>100) # 0
tdlist_sti <- split(sti_dtlink$drug_name, sti_dtlink$t_gn_sym)
tdnum_sti <- sapply(tdlist_sti, length)
sum(tdnum_sti>100) # 255
tdlist_sti <- tdlist_sti[tdnum_sti<=100]
tdlink_sti <- list2link(tdlist_sti, type="td")
dtlink_sti <- tdlink_sti[,c("drug_name","t_gn_sym")] # 50,012 X 2
length(unique(dtlink_sti$drug_name)) # 18,847
length(unique(dtlink_sti$t_gn_sym)) # 5,119
write.table(dtlink_sti, "./data/dtlink_sti_v5.0.xls", 
            quote=FALSE, sep="\t", row.names=FALSE)

## Get dtlink tables in DrugBank/CLUE/STITCH databases and store in 
## SQLite database
### dtlink: drug_name-t_gn_sym and drug_name-ENTREZ 
### (Note: drug_name in dtlink are all lowercase)
#### Combine dtlink in DrugBank/CLUE/STITCH databases
dtlink_db <- read.delim("data/dtlink_db_5.1.2.xls", stringsAsFactors=FALSE)
dtlink_clue <- read.delim("data/dtlink_clue_1.1.1.2.xls", stringsAsFactors=FALSE)
dtlink_sti <- read.delim("data/dtlink_sti_v5.0.xls", stringsAsFactors=FALSE)
dtlink <- rbind(dtlink_db, dtlink_clue, dtlink_sti) %>% unique
#### Exlude proteins or drugs that have more than 100 targets
dtlist <- split(dtlink$t_gn_sym, dtlink$drug_name)
tdlist <- split(dtlink$drug_name, dtlink$t_gn_sym)
dt_num <- sapply(dtlist, length)
sum(dt_num>100) # 0
td_num <- sapply(tdlist, length)
sum(td_num>100) # 29
tdlist <- tdlist[td_num<=100]
tdlink <- list2link(tdlist, type="td")
dtlink <- tdlink[,c("drug_name","t_gn_sym")] # 62,228 X 2
write.table(dtlink, "data/dtlink3.xls", quote=FALSE, row.names=FALSE, sep="\t")
length(unique(dtlink$drug_name)) # 22647
length(unique(dtlink$t_gn_sym)) # 5729
#### Map gene SYMBOL to entrez id to get drug_name-entrez links
library(org.Hs.eg.db)
sym <- AnnotationDbi::keys(org.Hs.eg.db, keytype = "SYMBOL")
sym_entrez <- AnnotationDbi::select(org.Hs.eg.db, keys = sym, 
                                    keytype = "SYMBOL", columns = "ENTREZID")
dtlink_entrez <- left_join(as_tibble(dtlink), sym_entrez, 
                           by = c("t_gn_sym"="SYMBOL")) %>% 
                 dplyr::select(drug_name, ENTREZID) %>% 
                 distinct(.keep_all = TRUE)
write.table(dtlink_entrez, "data/dtlink_entrez3.xls", quote=FALSE, 
            row.names=FALSE, sep="\t")
### Store dtlinks in SQLite database
library(RSQLite)
mydb <- dbConnect(SQLite(), "data/dtlink_db_clue_sti.db")
dbWriteTable(mydb, "dtlink_db", dtlink_db, overwrite = TRUE)
dbWriteTable(mydb, "dtlink_clue", dtlink_clue, overwrite = TRUE)
dbWriteTable(mydb, "dtlink_sti", dtlink_sti, overwrite = TRUE)
dbWriteTable(mydb, "dtlink", dtlink, overwrite = TRUE)
dbWriteTable(mydb, "dtlink_entrez", dtlink_entrez, overwrite = TRUE)
dbListTables(mydb)
dbDisconnect(mydb)

#####################
## make goAnno.rds ##
#####################

# It is an intermediate file storing the GO to GENE SYMBOLE mapping information
# for `dtnetplot` function in `signatureSearch` package

OrgDb <- GOSemSim::load_OrgDb(OrgDb = "org.Hs.eg.db")
kk <- keys(OrgDb, keytype="SYMBOL")
goAnno <- AnnotationDbi::select(OrgDb, keys=kk, keytype="SYMBOL",
                                columns=c("GOALL", "ONTOLOGYALL"))
goAnno <- unique(na.omit(goAnno)) # 3,430,396 X 4
saveRDS(goAnno, "./data/goAnno.rds")

##########################
## make goAnno_drug.rds ##
##########################

# It is an intermediate file storing the GO to drug name mapping information
# for `dsea` functions in `signatureSearch` package

## convert GOALL-SYMBOL in goAnno to GOALL-drug
### Get drug-target mapping information in DrugBank, CLUE and STITCH databases 
### from `dtlink_db_clue_sti.db` created above
library(RSQLite); library(dplyr)
conn <- dbConnect(SQLite(), "./data/dtlink_db_clue_sti.db")
dtlink <- dbGetQuery(conn, 'SELECT * FROM dtlink') # 62,228 X 2
dbDisconnect(conn)
### Convert GO-GENE mappings to GO-drug mappings
goAnno <- readRDS("./data/goAnno.rds")
goAnno_drug <- left_join(as_tibble(goAnno[,c(2,1,4)]), dtlink, 
                         by = c("SYMBOL"="t_gn_sym"))
goAnno_drug <- as.data.frame(goAnno_drug)[,c("GOALL","ONTOLOGYALL","drug_name")]
goAnno_drug <- na.omit(goAnno_drug)
goAnno_drug <- goAnno_drug[!duplicated(goAnno_drug[,c("GOALL","drug_name")]),]
saveRDS(goAnno_drug, "./data/goAnno_drug.rds") # 7,517,403 X 3

######################
## make GO_DATA.rds ##
######################

# It is an intermediate file storing the GO annotation environment (GO term to 
# gene sets, gene to GO terms, GO term ID to description, GO ID to ontology) 
# used for `tsea` methods to accelarate the speed by avoiding building this 
# environment every time running the tsea functions 

GO_DATA <- clusterProfiler:::get_GO_data(
  OrgDb="org.Hs.eg.db", ont="ALL", keytype="SYMBOL")
saveRDS(GO_DATA, "./data/GO_DATA.rds")

###########################
## make GO_DATA_drug.rds ##
###########################

# It is an intermediate file storing the GO annotation environment (GO term to 
# drug sets, drug to GO terms, GO term ID to description, GO ID to ontology) 
# used for `dsea` methods to accelarate the speed by avoiding building this 
# environment every time running the dsea functions 

## Read in `goAnno_drug.rds` generated above
goAnno_drug <- readRDS("data/goAnno_drug.rds")
GO2GENE <- goAnno_drug[,c("GOALL","drug_name")]
GO_DATA_drug <- clusterProfiler:::build_Anno(GO2GENE, 
                                  clusterProfiler:::get_GO2TERM_table())
saveRDS(GO_DATA_drug, "./data/GO_DATA_drug.rds")

#########################
## make taurefList.rds ##
#########################

# It uses signatures in the reference database (such as LINCS) to query against 
# itself as Qref to compute tau score of `gess_lincs` 
# method in `signatureSearch` package. Tau score compares observed enrichment 
# score to all others in Qref. Query results are 
# scored with tau as a standardized measure ranging from −100 to 100. A tau of 
# 90 indicates that only 10% of reference perturbations showed stronger 
# connectivity to the query. Tau represents the percentage of reference queries 
# with a lower |NCS| than |NCSq,r|, adjusted to retain the sign of NCSq,r. 
# NCSq,r is the normalized connectivity score for signature r relative to 
# query q. For more details, please refer to Subramanian et al., 2017, Cell
# (A Next Generation Connectivity Map: L1000 Platform and the First 1,000,000 Profiles)

## Create Query Reference DB for Tau Score Computation of `gess_lincs` method
### Load `lincs` database created above
library(HDF5Array); library(SummarizedExperiment)
lincs = loadHDF5SummarizedExperiment("./data/lincs")
queryReferenceDB <- function(se, Nup=150, Ndown=150, ES_NULL="Default", dest_path, partition) {
  dir = dirname(dest_path)
  score_mat = assay(se)
  ## Create query list for all signatures in se
  query_list <- lapply(colnames(score_mat), function(x) {
    sigvec = sort(as.matrix(score_mat[,x]), decreasing = TRUE)
    list(upset=utils::head(names(sigvec), Nup), downset=utils::tail(names(sigvec), Ndown))
  })
  names(query_list) = colData(se)$pert_cell_factor
  ## Define submission function
  f <- function(x, se, query_list, ES_NULL, dest_path) {
    chunkno <- x 
    sz <- 10 # small enough to use short queue 
    qlist <- split(query_list, ceiling(seq_along(names(query_list))/sz))
    myMA <- matrix(, length(query_list), sz, dimnames=list(names(query_list), seq_len(sz)))
    qlistone <- qlist[[chunkno]] 
    for(i in seq_along(qlistone)) {
      resultDF <- lincsEnrich(se, upset=qlistone[[i]]$up, downset=qlistone[[i]]$down, 
                              sortby=NA, output="no_tau", ES_NULL=ES_NULL, taurefList="Default")
      ncs <- resultDF$NCS
      mynames <- paste(resultDF$Pert, resultDF$Type, sep="__")
      mynames <- gsub("__up|__down", "", mynames)
      names(ncs) <- mynames
      myMA[,i] <- ncs[rownames(myMA)]
      colnames(myMA)[i] <- names(qlistone[i])
    }
    myMA <- myMA[, seq_along(qlistone)] # Only relevant for last entry that may not have as many colums as sz
    return(myMA)
  }
  ## Split query into chunks, each chunk will be processed on cluster as one process
  sz <- 10 # small enough to use short queue 
  qlist <- split(query_list, ceiling(seq_along(names(query_list))/sz)) 
  # qlist <-  qlist[seq_len(200)] # test
  # qlist <-  qlist[201:length(qlist)] # test
  dir = dirname(dest_path)
  setwd(dir)
  if(! file.exists("slurm.tmpl")) download.file("https://goo.gl/tLMddb", "slurm.tmpl", quiet=TRUE)
  if(! file.exists(".batchtools.conf.R")) download.file("https://goo.gl/5HrYkE", ".batchtools.conf.R", quiet=TRUE)
  if(dir.exists("tau_queries_reg")) unlink("tau_queries_reg", recursive=TRUE)
  reg = makeRegistry(file.dir="tau_queries_reg", conf.file=".batchtools.conf.R", packages="signatureSearch")
  ids = batchMap(f, x = seq(along=qlist), more.args = list(se=se, query_list=query_list, ES_NULL=ES_NULL, dest_path=dest_path))
  #testJob(id = 1)
  done <- submitJobs(ids, resources=list(walltime=43200, ncpus=1, memory=2048, partition=partition), reg=reg)
  print(waitForJobs())
  #getJobTable()
  print(getStatus())
  print(getErrorMessages())
  #showLog(1)
  # removeRegistry(wait=0, reg=reg)
  
  ## Inspect result and resubmit jobs for missing and empty files
  #dir = dirname(dest_path)
  #fileDF <- file.info(list.files(paste0(dir, "/tau_queries"), pattern="result_*", full.names=TRUE))
  #index_empty <- as.numeric(gsub(".*_", "", row.names(fileDF[fileDF$size==0, ])))
  #qlist <- split(query_list, ceiling(seq_along(names(query_list))/sz)) 
  #index_all_files <- seq_along(qlist)
  #index_exist <- as.numeric(gsub(".*_", "", row.names(fileDF)))
  #index_missing <- index_all_files[!index_all_files %in% index_exist]
  #index_repeat <- unique(sort(c(index_empty, index_missing)))
  #if(length(index_repeat)!=0) outlist <- bplapply(index_repeat, f)
  
  ## Organize results in list where each component contains data.frame with query results from one cell type
  if(waitForJobs){
    pathDF <- data.frame(query=names(query_list), path=rep(seq_along(qlist), each=sz))
    pathDF <- data.frame(pathDF, target=gsub("^.*?__", "", pathDF$query))
    pathList <- split(as.character(pathDF$path), factor(pathDF$target))
    pathList <- vapply(pathList, unique) # eliminates unnecessary/duplicated imports of files
    taurefList <- rep(NA, length(pathList)); names(taurefList) <- names(pathList)
    taurefList <- as.list(taurefList) 
    for(celltype in names(pathList)) {
      for(i in seq_along(pathList[[celltype]])) {
        tmpDF <- loadResult(as.numeric(pathList[[celltype]][i]))
        tmpDF <- round(tmpDF, 2) # Reduces data size
        colindex <- gsub("^.*?__", "", colnames(tmpDF)) %in% celltype
        tmpDF <- tmpDF[, colindex, drop=FALSE] 
        if(i==1) { 
          rowindex <- gsub("^.*?__", "", rownames(tmpDF)) %in% celltype
          containerDF <- tmpDF[rowindex, , drop=FALSE]
        } else {
          containerDF <- cbind(containerDF, tmpDF[rownames(containerDF),])
        }
        print(paste("Finished", i, "of", length(pathList[[celltype]]), "results collected in", ncol(containerDF), "columns."))
      }
      taurefList[[celltype]] <- containerDF
      rm(containerDF); gc()
    }
    saveRDS(taurefList, file=dest_path)
  } else {
    print("Not all the submitted jobs are successfully completed, please check!")
  }
}
## Create query list, here all signatures in 
lincsvolumes <- list.files("data/cmapdbprofiles", pattern="*GSE92742*", full.names=TRUE) # Uses only TouchstoneV1 = CMap-L1000v1 = GSE92742
query_list <- queryList(volumes=lincsvolumes, Nup=150, Ndown=150) 
saveRDS(query_list, "data/tau_queries/query_list.rds")
## Define submission function
f <- function(x) {
  source("/rhome/tgirke/Projects/longevity/cmapEnrichmentFct/cmapVolSearch_Fct.R")
  chunkno <- x 
  lincsvolumes <- list.files("data/cmapdbprofiles", pattern="*GSE92742*", full.names=TRUE) # Uses only TouchstoneV1 = CMap-L1000v1 = GSE92742
  query_list <- readRDS("data/tau_queries/query_list.rds") 
  sz <- 10 # small enough to use short queue 
  qlist <- split(query_list, ceiling(seq_along(names(query_list))/sz)) 
  myMA <- matrix(, length(query_list), sz, dimnames=list(names(query_list), 1:sz))
  qlistone <- qlist[[chunkno]] 
  for(i in seq_along(qlistone)) {
    resultDF <- lincsEnrichByVolume(lincsvolumes, upset=qlistone[[i]]$up, downset=qlistone[[i]]$down, sortby=NA)
    ncs <- resultDF$NCS
    mynames <- paste(resultDF$Pert, resultDF$Type, sep="__")
    mynames <- gsub("__up|__down", "", mynames)
    names(ncs) <- mynames
    myMA[,i] <- ncs[rownames(myMA)]
    colnames(myMA)[i] <- names(qlistone[i])
  }
  myMA <- myMA[, seq_along(qlistone)] # Only relevant for last entry that may not have as many colums as sz
  write.table(as.data.frame(myMA), file=paste0("data/tau_queries/result_", sprintf("%03d", chunkno)), col.names = NA, quote=FALSE, sep="\t")
}
## Split query into chunks, each chunk will be processed on cluster as one process
query_list <- readRDS("data/tau_queries/query_list.rds") 
sz <- 10 # small enough to use short queue 
qlist <- split(query_list, ceiling(seq_along(names(query_list))/sz)) 
library(BiocParallel); library(BatchJobs)
# qlist <-  qlist[1:200] # test
# qlist <-  qlist[201:length(qlist)] # test
funs <- makeClusterFunctionsSLURM("slurm.tmpl")
param <- BatchJobsParam(length(qlist), resources=list(walltime="1:50:00", ntasks=1, ncpus=1, memory="10gb"), cluster.functions=funs)
register(param)
outlist <- bplapply(seq(along=qlist), f)

## Inspect result and resubmit jobs for missing and empty files
fileDF <- file.info(list.files("data/tau_queries/", "result_*", full.names=TRUE)) 
index_empty <- as.numeric(gsub(".*_", "", row.names(fileDF[fileDF$size==0, ])))
qlist <- split(query_list, ceiling(seq_along(names(query_list))/sz)) 
index_all_files <- seq_along(qlist)
index_exist <- as.numeric(gsub(".*_", "", row.names(fileDF)))
index_missing <- index_all_files[!index_all_files %in% index_exist]
index_repeat <- unique(sort(c(index_empty, index_missing)))
if(length(index_repeat)!=0) outlist <- bplapply(index_repeat, f)

## Organize results in list where each component contains data.frame with query results from one cell type
pathDF <- data.frame(query=names(query_list), path=rep(paste0("data/tau_queries/result_", sprintf("%03d", seq_along(qlist))), sapply(qlist, length)))
pathDF <- data.frame(pathDF, target=gsub("^.*?__", "", pathDF$query))
pathList <- split(as.character(pathDF$path), factor(pathDF$target))
pathList <- sapply(pathList, unique) # eliminates unnecessary/duplicated imports of files
taurefList <- rep(NA, length(pathList)); names(taurefList) <- names(pathList)
taurefList <- as.list(taurefList) 
for(celltype in names(pathList)) {
  for(i in seq_along(pathList[[celltype]])) {
    tmpDF <- read.delim(pathList[[celltype]][i], row.names=1, check.names=FALSE)
    tmpDF <- round(tmpDF, 2) # Reduces data size
    colindex <- gsub("^.*?__", "", colnames(tmpDF)) %in% celltype
    tmpDF <- tmpDF[, colindex, drop=FALSE] 
    if(i==1) { 
      rowindex <- gsub("^.*?__", "", rownames(tmpDF)) %in% celltype
      containerDF <- tmpDF[rowindex, , drop=FALSE]
    } else {
      containerDF <- cbind(containerDF, tmpDF[rownames(containerDF),])
    }
    print(paste("Finished", i, "of", length(pathList[[celltype]]), "files collected in", ncol(containerDF), "columns."))
  }
  taurefList[[celltype]] <- containerDF
  saveRDS(taurefList, file="data/tau_queries/taurefList.rds")
  rm(containerDF); gc()
}
#' @param se `SummarizedExperiment` object represents refernce database
#' @param Nup number of up-regulated genes subsetted for each signature in reference database `se` as query signature
#' @param Ndown number of down-regulated genes subsetted for each signature in reference database `se` as query signature
#' @param ES_NULL path to the ES_NULL file. ES_null distribution is generated with random queryies for computing nominal P-values for ES by 
#' `randQueryES_slurm` or `randQueryES_local` function. If `ES_NULL` is set as `Default`, it uses the ES_null distribution that we generated.
#' @param dest_path path to the destination file, including the file name "taurefList.rds"
#' @param partition partition node used to submit jobs on cluster
#' @return creat "taurefList.rds" file
#' @export